{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted xml to csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "val_image_path = './training_all'\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = ('soccer/notebooks/'+val_image_path + '/' + root.find('filename').text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text),\n",
    "                    #  int(root.find('size')[0].text),\n",
    "                    #  int(root.find('size')[1].text),\n",
    "                     member[0].text\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['path_to_image', 'xmin', 'ymin', 'xmax', 'ymax', 'class']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    image_path = os.path.join(os.getcwd(), 'Annotations')\n",
    "    xml_df = xml_to_csv(val_image_path)\n",
    "    xml_df.to_csv('annotations.csv', index=None)\n",
    "    print('Successfully converted xml to csv.')\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "def generate_class_mapping():\n",
    "    class_map_list = []\n",
    "    fieldnames =  ['path_to_image','xmin','ymin','xmax','ymax','class']\n",
    "    i = 0\n",
    "    with open('./annotations.csv', 'r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file, fieldnames=fieldnames)\n",
    "        next(reader)\n",
    "        for line, row in enumerate(reader):\n",
    "            #print(row['class'])\n",
    "            value = (row['class'], i)\n",
    "            i += 1\n",
    "            class_map_list.append(value)\n",
    "    column_name = ['class', 'id']\n",
    "    df = pd.DataFrame(class_map_list, columns=column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = generate_class_mapping()\n",
    "    df.to_csv('class_mappings.csv', index=None)\n",
    "    # image_path = os.path.join(os.getcwd(), 'Annotations')\n",
    "    # xml_df = xml_to_csv(image_path)\n",
    "    # xml_df.to_csv('annotations.csv', index=None)\n",
    "    # print('Successfully converted xml to csv.')\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>player</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ball</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>referee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goalie</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manager</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  id\n",
       "0   player   0\n",
       "1     ball   1\n",
       "2  referee   2\n",
       "3   goalie   3\n",
       "4  manager   4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = generate_class_mapping()\n",
    "df = pd.unique(df['class'])\n",
    "df = pd.DataFrame({'class':df,'id':np.arange(len(df))})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('class_mappings.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n",
    "mb = master_bar(range(10))\n",
    "\n",
    "for i in range(0,100):\n",
    "    for j in range(0,50):\n",
    "        sleep(0.001)\n",
    "    mb.update(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6fb328b56927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastprogress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'display'"
     ]
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n",
    "mb = master_bar(range(10))\n",
    "for i in mb:\n",
    "    for j in progress_bar(range(100), parent=mb):\n",
    "        sleep(0.01)\n",
    "        mb.child.comment = f'second bar stat'\n",
    "    mb.first_bar.comment = f'first bar stat'\n",
    "    mb.write(f'Finished loop {i}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ewaymao/retinanet-pytorch-1.0/soccer/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ewaymao/retinanet-pytorch-1.0\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Elapsed time: 2.373469591140747\n",
      "Elapsed time: 2.402557611465454\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pdb\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\n",
    "\n",
    "\n",
    "#assert torch.__version__.split('.')[1] == '4'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Simple training script for training a RetinaNet network.')\n",
    "\n",
    "    #parser.add_argument('--dataset', help='Dataset type, must be one of csv or coco.')\n",
    "    #parser.add_argument('--coco_path', help='Path to COCO directory')\n",
    "    #parser.add_argument('--csv_classes', help='Path to file containing class list (see readme)')\n",
    "    #parser.add_argument('--csv_val', help='Path to file containing validation annotations (optional, see readme)')\n",
    "\n",
    "    #parser.add_argument('--model', help='Path to model (.pt) file.')\n",
    "    \n",
    "    #parser = parser.parse_args(args)\n",
    "    parser.dataset = 'csv'\n",
    "    parser.csv_classes = 'soccer/notebooks/class_mappings.csv'\n",
    "    parser.csv_val = 'soccer/notebooks/annotations.csv'\n",
    "    parser.model = 'model_final.pt'\n",
    "    \n",
    "    if parser.dataset == 'coco':\n",
    "        dataset_val = CocoDataset(parser.coco_path, set_name='val2017', transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "    elif parser.dataset == 'csv':\n",
    "        dataset_val = CSVDataset(train_file=parser.csv_val, class_list=parser.csv_classes, transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "    else:\n",
    "        raise ValueError('Dataset type not understood (must be csv or coco), exiting.')\n",
    "\n",
    "    sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=1, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "    retinanet = torch.load(parser.model)\n",
    "    #retinanet = model.resnet50(num_classes=80,)\n",
    "    #retinanet.load_state_dict(torch.load('coco_resnet50_statedict.pt'))\n",
    "\n",
    "    retinanet.eval()\n",
    "\n",
    "    unnormalize = UnNormalizer()\n",
    "\n",
    "    def draw_caption(image, box, caption):\n",
    "\n",
    "        b = np.array(box).astype(int)\n",
    "        cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
    "        cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for idx, data in enumerate(dataloader_val):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            st = time.time()\n",
    "            scores, classification, transformed_anchors = retinanet(data['img'].float())\n",
    "            print('Elapsed time: {}'.format(time.time()-st))\n",
    "\n",
    "            #breakpoint()\n",
    "            if idx >= 1: break\n",
    "            \n",
    "            idxs = np.where(scores>0.1)                                                    \n",
    "            img = np.array(255 * unnormalize(data['img'][0, :, :, :])).copy()\n",
    "\n",
    "            img[img<0] = 0\n",
    "            img[img>255] = 255\n",
    "\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "            img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            for j in range(idxs[0].shape[0]):\n",
    "                bbox = transformed_anchors[idxs[0][j], :]\n",
    "                x1 = int(bbox[0])\n",
    "                y1 = int(bbox[1])\n",
    "                x2 = int(bbox[2])\n",
    "                y2 = int(bbox[3])\n",
    "                label_name = dataset_val.labels[int(classification[idxs[0][j]])]\n",
    "                draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n",
    "                print(label_name)\n",
    "\n",
    "            cv2.imshow('img', img)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ewaymao/retinanet-pytorch-1.0/soccer/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10801.jpg  16531.xml  22621.jpg  3501.xml  4251.jpg  5801.xml  8701.jpg\r\n",
      "10801.xml  17401.jpg  22621.xml  3601.jpg  4251.xml  6091.jpg  8701.xml\r\n",
      "14211.jpg  17401.xml  241.jpg\t 3601.xml  4641.jpg  6091.xml  8881.jpg\r\n",
      "14211.xml  17691.jpg  241.xml\t 3751.jpg  4641.xml  6381.jpg  8881.xml\r\n",
      "14501.jpg  17691.xml  2611.jpg\t 3751.xml  4931.jpg  6381.xml  8991.jpg\r\n",
      "14501.xml  17981.jpg  2611.xml\t 3841.jpg  4931.xml  6961.jpg  8991.xml\r\n",
      "14791.jpg  17981.xml  2751.jpg\t 3841.xml  5281.jpg  6961.xml  9281.jpg\r\n",
      "14791.xml  18271.jpg  2751.xml\t 4001.jpg  5281.xml  7831.jpg  9281.xml\r\n",
      "15081.jpg  18271.xml  3001.jpg\t 4001.xml  5511.jpg  7831.xml  9571.jpg\r\n",
      "15081.xml  2001.jpg   3001.xml\t 4061.jpg  5511.xml  8411.jpg  9571.xml\r\n",
      "15371.jpg  2001.xml   3251.jpg\t 4061.xml  5751.jpg  8411.xml\r\n",
      "15371.xml  20881.jpg  3251.xml\t 4081.jpg  5751.xml  8641.jpg\r\n",
      "16531.jpg  20881.xml  3501.jpg\t 4081.xml  5801.jpg  8641.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls training_all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls training_all/ | grep xml > xml_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10801.xml\r\n",
      "14211.xml\r\n",
      "14501.xml\r\n",
      "14791.xml\r\n",
      "15081.xml\r\n",
      "15371.xml\r\n",
      "16531.xml\r\n",
      "17401.xml\r\n",
      "17691.xml\r\n",
      "17981.xml\r\n",
      "18271.xml\r\n",
      "2001.xml\r\n",
      "20881.xml\r\n",
      "22621.xml\r\n",
      "241.xml\r\n",
      "2611.xml\r\n",
      "2751.xml\r\n",
      "3001.xml\r\n",
      "3251.xml\r\n",
      "3501.xml\r\n",
      "3601.xml\r\n",
      "3751.xml\r\n",
      "3841.xml\r\n",
      "4001.xml\r\n",
      "4061.xml\r\n",
      "4081.xml\r\n",
      "4251.xml\r\n",
      "4641.xml\r\n",
      "4931.xml\r\n",
      "5281.xml\r\n",
      "5511.xml\r\n",
      "5751.xml\r\n",
      "5801.xml\r\n",
      "6091.xml\r\n",
      "6381.xml\r\n",
      "6961.xml\r\n",
      "7831.xml\r\n",
      "8411.xml\r\n",
      "8641.xml\r\n",
      "8701.xml\r\n",
      "8881.xml\r\n",
      "8991.xml\r\n",
      "9281.xml\r\n",
      "9571.xml\r\n"
     ]
    }
   ],
   "source": [
    "!cat xml_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-3-b13a5db17d03>(119)<module>()\n",
      "-> for line in f:\n",
      "(Pdb) n\n",
      "> <ipython-input-3-b13a5db17d03>(120)<module>()\n",
      "-> fileName = line.strip().decode('ascii')\n",
      "(Pdb) c\n",
      "10801.xml\n",
      "14211.xml\n",
      "14501.xml\n",
      "14791.xml\n",
      "15081.xml\n",
      "15371.xml\n",
      "16531.xml\n",
      "17401.xml\n",
      "17691.xml\n",
      "17981.xml\n",
      "18271.xml\n",
      "2001.xml\n",
      "20881.xml\n",
      "22621.xml\n",
      "241.xml\n",
      "2611.xml\n",
      "2751.xml\n",
      "3001.xml\n",
      "3251.xml\n",
      "3501.xml\n",
      "3601.xml\n",
      "3751.xml\n",
      "3841.xml\n",
      "4001.xml\n",
      "4061.xml\n",
      "4081.xml\n",
      "4251.xml\n",
      "4641.xml\n",
      "4931.xml\n",
      "5281.xml\n",
      "5511.xml\n",
      "5751.xml\n",
      "5801.xml\n",
      "6091.xml\n",
      "6381.xml\n",
      "6961.xml\n",
      "7831.xml\n",
      "8411.xml\n",
      "8641.xml\n",
      "8701.xml\n",
      "8881.xml\n",
      "8991.xml\n",
      "9281.xml\n",
      "9571.xml\n",
      "File Name: 10801.xml and image_id 1\n",
      "File Name: 14211.xml and image_id 2\n",
      "File Name: 14501.xml and image_id 3\n",
      "File Name: 14791.xml and image_id 4\n",
      "File Name: 15081.xml and image_id 5\n",
      "File Name: 15371.xml and image_id 6\n",
      "File Name: 16531.xml and image_id 7\n",
      "File Name: 17401.xml and image_id 8\n",
      "File Name: 17691.xml and image_id 9\n",
      "File Name: 17981.xml and image_id 10\n",
      "File Name: 18271.xml and image_id 11\n",
      "File Name: 2001.xml and image_id 12\n",
      "File Name: 20881.xml and image_id 13\n",
      "File Name: 22621.xml and image_id 14\n",
      "File Name: 241.xml and image_id 15\n",
      "File Name: 2611.xml and image_id 16\n",
      "File Name: 2751.xml and image_id 17\n",
      "File Name: 3001.xml and image_id 18\n",
      "File Name: 3251.xml and image_id 19\n",
      "File Name: 3501.xml and image_id 20\n",
      "File Name: 3601.xml and image_id 21\n",
      "File Name: 3751.xml and image_id 22\n",
      "File Name: 3841.xml and image_id 23\n",
      "File Name: 4001.xml and image_id 24\n",
      "File Name: 4061.xml and image_id 25\n",
      "File Name: 4081.xml and image_id 26\n",
      "File Name: 4251.xml and image_id 27\n",
      "File Name: 4641.xml and image_id 28\n",
      "File Name: 4931.xml and image_id 29\n",
      "File Name: 5281.xml and image_id 30\n",
      "File Name: 5511.xml and image_id 31\n",
      "File Name: 5751.xml and image_id 32\n",
      "File Name: 5801.xml and image_id 33\n",
      "File Name: 6091.xml and image_id 34\n",
      "File Name: 6381.xml and image_id 35\n",
      "File Name: 6961.xml and image_id 36\n",
      "File Name: 7831.xml and image_id 37\n",
      "File Name: 8411.xml and image_id 38\n",
      "File Name: 8641.xml and image_id 39\n",
      "File Name: 8701.xml and image_id 40\n",
      "File Name: 8881.xml and image_id 41\n",
      "File Name: 8991.xml and image_id 42\n",
      "File Name: 9281.xml and image_id 43\n",
      "File Name: 9571.xml and image_id 44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import json\n",
    "from xml.dom import minidom\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "#attrDict = {\"images\":[{\"file_name\":[],\"height\":[], \"width\":[],\"id\":[]}], \"type\":\"instances\", \"annotations\":[], \"categories\":[]}\n",
    "\n",
    "#xmlfile = \"000023.xml\"\n",
    "\n",
    "\n",
    "def generateVOC2Json(rootDir,xmlFiles):\n",
    "    #breakpoint()\n",
    "    attrDict = dict()\n",
    "    #images = dict()\n",
    "    #images1 = list()\n",
    "    attrDict[\"categories\"]=[{\"supercategory\":\"none\",\"id\":0,\"name\":\"player\"},\n",
    "                    {\"supercategory\":\"none\",\"id\":1,\"name\":\"ball\"},\n",
    "                    {\"supercategory\":\"none\",\"id\":2,\"name\":\"referee\"},\n",
    "                    {\"supercategory\":\"none\",\"id\":3,\"name\":\"goalie\"},\n",
    "                {\"supercategory\":\"none\",\"id\":4,\"name\":\"manager\"},\n",
    "                {\"supercategory\":\"none\",\"id\":5,\"name\":\"cameraman\"}]\n",
    "    images = list()\n",
    "    annotations = list()\n",
    "    for root, dirs, files in os.walk(rootDir):\n",
    "        image_id = 0\n",
    "        for file in xmlFiles:\n",
    "            image_id = image_id + 1\n",
    "            \n",
    "            #breakpoint()\n",
    "            if file in files:\n",
    "                \n",
    "                #image_id = image_id + 1\n",
    "                annotation_path = os.path.abspath(os.path.join(root, file))\n",
    "                \n",
    "                #tree = ET.parse(annotation_path)#.getroot()\n",
    "                image = dict()\n",
    "                #keyList = list()\n",
    "                doc = xmltodict.parse(open(annotation_path).read())\n",
    "                #print doc['annotation']['filename']\n",
    "                image['file_name'] = str(doc['annotation']['filename'])\n",
    "                #keyList.append(\"file_name\")\n",
    "                image['height'] = int(doc['annotation']['size']['height'])\n",
    "                #keyList.append(\"height\")\n",
    "                image['width'] = int(doc['annotation']['size']['width'])\n",
    "                #keyList.append(\"width\")\n",
    "\n",
    "                #image['id'] = str(doc['annotation']['filename']).split('.jpg')[0]\n",
    "                image['id'] = image_id\n",
    "                print(\"File Name: {} and image_id {}\".format(file, image_id))\n",
    "                images.append(image)\n",
    "                # keyList.append(\"id\")\n",
    "                # for k in keyList:\n",
    "                #     images1.append(images[k])\n",
    "                # images2 = dict(zip(keyList, images1))\n",
    "                # print images2\n",
    "                #print images\n",
    "\n",
    "                #attrDict[\"images\"] = images\n",
    "\n",
    "                #print attrDict\n",
    "                #annotation = dict()\n",
    "                id1 = 1\n",
    "                if 'object' in doc['annotation']:\n",
    "                    for obj in doc['annotation']['object']:\n",
    "                        for value in attrDict[\"categories\"]:\n",
    "                            annotation = dict()\n",
    "                            #if str(obj['name']) in value[\"name\"]:\n",
    "                            if str(obj['name']) == value[\"name\"]:\n",
    "                                #print str(obj['name'])\n",
    "                                #annotation[\"segmentation\"] = []\n",
    "                                annotation[\"iscrowd\"] = 0\n",
    "                                #annotation[\"image_id\"] = str(doc['annotation']['filename']).split('.jpg')[0] #attrDict[\"images\"][\"id\"]\n",
    "                                annotation[\"image_id\"] = image_id\n",
    "                                x1 = int(obj[\"bndbox\"][\"xmin\"])  - 1\n",
    "                                y1 = int(obj[\"bndbox\"][\"ymin\"]) - 1\n",
    "                                x2 = int(obj[\"bndbox\"][\"xmax\"]) - x1\n",
    "                                y2 = int(obj[\"bndbox\"][\"ymax\"]) - y1\n",
    "                                annotation[\"bbox\"] = [x1, y1, x2, y2]\n",
    "                                annotation[\"area\"] = float(x2 * y2)\n",
    "                                annotation[\"category_id\"] = value[\"id\"]\n",
    "                                annotation[\"ignore\"] = 0\n",
    "                                annotation[\"id\"] = id1\n",
    "                                annotation[\"segmentation\"] = [[x1,y1,x1,(y1 + y2), (x1 + x2), (y1 + y2), (x1 + x2), y1]]\n",
    "                                id1 +=1\n",
    "\n",
    "                                annotations.append(annotation)\n",
    "                \n",
    "                else:\n",
    "                    print(\"File: {} doesn't have any object\".format(file))\n",
    "                #image_id = image_id + 1\n",
    "                \n",
    "            else:\n",
    "                print(\"File: {} not found\".format(file))\n",
    "            \n",
    "\n",
    "    attrDict[\"images\"] = images    \n",
    "    attrDict[\"annotations\"] = annotations\n",
    "    attrDict[\"type\"] = \"instances\"\n",
    "\n",
    "    #print attrDict\n",
    "    jsonString = json.dumps(attrDict)\n",
    "    with open(\"soccer_annotations.json\", \"w\") as f:\n",
    "        f.write(jsonString)\n",
    "\n",
    "# rootDir = \"/netscratch/pramanik/OBJECT_DETECTION/detectron/lib/datasets/data/Receipts/Annotations\"\n",
    "# for root, dirs, files in os.walk(rootDir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".xml\"):\n",
    "#             annotation_path = str(os.path.abspath(os.path.join(root,file)))\n",
    "#             #print(annotation_path)\n",
    "#             generateVOC2Json(annotation_path)\n",
    "trainFile = \"xml_list\"\n",
    "trainXMLFiles = list()\n",
    "with open(trainFile, \"rb\") as f:\n",
    "    #breakpoint()\n",
    "    for line in f:\n",
    "        fileName = line.strip().decode('ascii')\n",
    "        print(fileName)\n",
    "        trainXMLFiles.append(fileName)\n",
    "\n",
    "\n",
    "rootDir = \"./training_all/\"\n",
    "generateVOC2Json(rootDir, trainXMLFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
